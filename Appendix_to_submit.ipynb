{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><center>Appendix - Text Mining - Replicating Amazon X-Ray using Machine Learning</center></h1>\n",
    "<img src=\"harvardlogo.jpg\", width=100, height=100>\n",
    "<p><b><center>Harvard University</center></b></p>\n",
    "<p><b><center>CSCI E-81 Machine Learning & Data Mining</center></b></p>\n",
    "<p><b><center>Fall 2016</center></b></p>\n",
    "<p><b><center>Team: Nirmal Labh, Anmol Joshi</center></b></p>\n",
    "<p><b><center>Due Date: Monday, December 17th, 2016 at 11:59pm</center></b></p>\n",
    "<p><b><center>Submission Date: Saturday, December 17th, 2016</center></b></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"trivia.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**We experimented with some future work before submission. We used one of my favorite movies, Swingers, starring Jon Faverau and Vince Vaughn. **\n",
    "\n",
    "**IMDb has very stricy policies on web scraping and any existing IMDb APIs do not have downloading trivia, references and goofs features. We found one, but currently that is not working for them, and will be updated on the next blockpoint. So for this I went to IMDb and copied the text in to a notepad file, we'll have a better version of figuring out how to extract that information in the future.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import pysrt\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "import urllib\n",
    "import gensim\n",
    "import nltk.data\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import cm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMDb lists references and trivia, IMDb sometimes simply lists the movie name, this can refer to how a scene is directed, music of that scene. So for this project, we're simply seeing where a character talks about a certain movie, uses a quote, basically related purely to text for that movie.** \n",
    "\n",
    "**In the future, we would like to be able to have this model automated to extract information from different movies pages to accurately classify text**\n",
    "\n",
    "**The issue with this project is that there is no training and testing set, that's why I used a movie I know well so I'm able to confirm all the references**\n",
    "\n",
    "**We did not attempt trivia, given the time constraint, but how we imagine it would work is to parse through the movie script, so we can get scene descriptions which would help find what trivia section applies to what point of the movie**\n",
    "\n",
    "**We download the subtitles and display it so it shows the start and end time of the subtitles on screen, with the subtitle**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>end_time</th>\n",
       "      <th>start_time</th>\n",
       "      <th>subtitles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14</td>\n",
       "      <td>11</td>\n",
       "      <td>No way.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41</td>\n",
       "      <td>33</td>\n",
       "      <td>You're nobody 'til somebody loves you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>46</td>\n",
       "      <td>41</td>\n",
       "      <td>You're nobody 'til somebody cares</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>54</td>\n",
       "      <td>48</td>\n",
       "      <td>You may be king You may possess the world</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>56</td>\n",
       "      <td>54</td>\n",
       "      <td>And its gold</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   end_time  start_time                                  subtitles\n",
       "0        14          11                                    No way.\n",
       "1        41          33      You're nobody 'til somebody loves you\n",
       "2        46          41          You're nobody 'til somebody cares\n",
       "3        54          48  You may be king You may possess the world\n",
       "4        56          54                               And its gold"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subs = pysrt.open('Swingers (1996).DVDRip.DivX.English.srt', encoding='iso-8859-1')\n",
    "\n",
    "n = len(subs)\n",
    "subs_start_time = []\n",
    "subs_end_time = []\n",
    "subs_txt = []\n",
    "\n",
    "for ii in range(0, n):\n",
    "    subs_start_time.append(3600*subs[ii].start.hours + 60*subs[ii].start.minutes + subs[ii].start.seconds)\n",
    "    subs_end_time.append(3600*subs[ii].end.hours + 60*subs[ii].end.minutes + subs[ii].end.seconds)\n",
    "    L = subs[ii].text\n",
    "    L = L.replace(\"\\n\", \" \")\n",
    "    subs_txt.append(L)    \n",
    "    \n",
    "d = {'start_time': subs_start_time, 'end_time': subs_end_time, 'subtitles': subs_txt}\n",
    "df = pd.DataFrame(data=d)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There was no available public API to get trivia, references and goofs from IMDb. Given IMDb's web scraping policy, we avoided scraping directly from there. Instead we saved the trivia to a text file. We can look into scraping in the future. \n",
    "\n",
    "We now read the saved trivia file, that was mentioned earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "f = open('swingers_trivia.txt')\n",
    "trivia = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for tr in f.readlines():\n",
    "    trivia.append(tr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use word2vec for this problem statement. \n",
    "\n",
    "word2vec was an algorithm created at Google. It uses a shallow 2 layer neural network to assign vector representations of words by forming linguistic context of words. We'll see later in these sections how these word embeddings and context of words.\n",
    "\n",
    "word2vec works well here converts text to a vector representation to the size of one's choosing and trained on text of our choosing. word2vec is the current standard for essentially a dimensionality reduction for text. The lower space groups similar words together, which improves modeling for both supervised and unsupervised learning methods.\n",
    "\n",
    "We start by converting the trivia, subtitles to a bag of words models. This works by removing all the stop words, for example: the, a , an, by, etc. Although, for this we choose not to remove stop words, as certain subtitles can just be one word. \n",
    "\n",
    "These are mainly articles, prepositions, pronouns. After having removed the stopwords, we create a bag of words by creating a vocabulary of words and then create feature vectors of sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def to_wordlist(text, remove_stopwords):\n",
    "    \n",
    "    text = re.sub(\"[^a-zA-Z]\", \" \", text)\n",
    "    \n",
    "    words = text.lower().split()\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        words = [w for w in words if not w in stops]\n",
    "    \n",
    "    return (words)\n",
    "\n",
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "def to_sentences(text, remove_stopwords):\n",
    "    \n",
    "    raw_sentences = tokenizer.tokenize(text.strip())\n",
    "    \n",
    "    sentences = []\n",
    "    \n",
    "    for raw_sentence in raw_sentences:\n",
    "        \n",
    "        if len(raw_sentence) > 0:\n",
    "            \n",
    "            sentences.append(to_wordlist(raw_sentence, remove_stopwords))\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our analysis, we have to create averaged feature vectors of the words of each string. We do this by using two functions makeFeatureVec and getAvgFeatureVecs.\n",
    "makeFeatureVec takes text and converts each word to vector representation using the word2vec model. Each word vector representation is added together and then averaged by diving by the number of words of the text. This way each essay results in a vector with 300 features that is an average of each of the words in the text.\n",
    "getAvgFeatureVecs creates vectors for text for the entire dataset using makeFeatureVec function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def makeFeatureVec(words, model, num_features):\n",
    "    featureVec = np.zeros((num_features,),dtype=\"float32\")\n",
    "    nwords = 0.\n",
    "    index2word_set = set(model.index2word)\n",
    "    for word in words:\n",
    "        if word in index2word_set: \n",
    "            nwords = nwords + 1.\n",
    "            featureVec = np.add(featureVec,model[word])        \n",
    "    featureVec = np.divide(featureVec,nwords)\n",
    "    return featureVec\n",
    "\n",
    "def getAvgFeatureVecs(text, model, num_features):\n",
    "    counter = 0.\n",
    "    essayFeatureVecs = np.zeros((len(text),num_features),dtype=\"float32\")\n",
    "    for tr in text:\n",
    "        essayFeatureVecs[counter] = makeFeatureVec(tr, model, num_features)\n",
    "        counter = counter + 1.\n",
    "    return essayFeatureVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from trivia set\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "sentences_tr = []\n",
    "\n",
    "print (\"Parsing sentences from trivia set\")\n",
    "for text in trivia:    \n",
    "    sentences_tr += to_sentences(text, remove_stopwords = False)\n",
    "\n",
    "print (\"Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use Google's pre-trained model on Google News to vectorize our text. We found to be more useful and general that training the model on text for each movie, as that would require constant training. Here is a link to the model https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load Google's pre-trained Word2Vec model.\n",
    "model = gensim.models.Word2Vec.load_word2vec_format('GoogleNews-vectors-negative300.bin', binary=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I tried running this block of code below, but it would hang my computer. So I copied the image below from a project from my other class - CS109A Introduction to Data Science. Here is the project page - https://projectaes.github.io/\n",
    "\n",
    "Below is an example of word embedding from Google's model. \n",
    "It is important to examine word embedding and see how words cluster together based on Google's word2vec model. To visualize this, we'll apply Prinicipal Component Analysis and reduce the word dimensionality to 2 componenets. We use the transformed word vectors and represent them on a 2D plot. We examine the top 25 most similar words to the word \"time\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def viswordembedding(word_to_use, topn_):\n",
    "    okay = model.most_similar(word_to_use, topn = topn_)\n",
    "    words_to_show = [word_to_use]\n",
    "    words_vec = np.zeros((topn_+1, 300))\n",
    "    words_vec[0,:] = model[word_to_use]\n",
    "\n",
    "    for ii in range(0, len(okay)):\n",
    "        words_to_show.append(okay[ii][0])    \n",
    "        words_vec[ii+1,:] = model[okay[ii][0]]\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    pca.fit(words_vec)\n",
    "    X = pca.transform(words_vec)\n",
    "    xs = X[:, 0]\n",
    "    ys = X[:, 1]\n",
    "\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.scatter(xs, ys, marker = 'o')\n",
    "    plt.xlabel('PCA1')\n",
    "    plt.ylabel('PCA2')\n",
    "    plt.title('Word Embedding Nearest Neighbors for ' + word_to_use)\n",
    "    for i, w in enumerate(words_to_show):\n",
    "        plt.annotate(\n",
    "            w,\n",
    "            xy = (xs[i], ys[i]), xytext = (3, 3),\n",
    "            textcoords = 'offset points', ha = 'left', va = 'top')\n",
    "    plt.savefig('wordembedding', bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"wordembedding.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we examine the 25 nearest neighbors to the word \"time\". What's most interesting is that the units of time cluster together in left of the plot i.e. hours, days, weeks, years. This is all decided by context of the words!\n",
    "A few other clusters are activities such as exercising, enjoying, outdoors - in the lower right corner of the graph.\n",
    "What's most interesting is the top most clusters. the words patience, patient cluster together as nearest neighbours for time!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we create feature vectors of all the subtitles and all of the trivia. We'll display it to show you how this text is being represented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating average feature vecs for Trivia\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshi/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/__main__.py:9: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "print (\"Creating average feature vecs for Trivia\")\n",
    "clean_trivia = []\n",
    "for text in trivia:\n",
    "    clean_trivia.append(to_wordlist(text, remove_stopwords= False ))\n",
    "triviaDataVecs = getAvgFeatureVecs(clean_trivia, model, num_features = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.00483032,  0.01160645, -0.00051758, ..., -0.06493653,\n",
       "        -0.01777466,  0.04527344],\n",
       "       [-0.00179499,  0.03970064,  0.03865051, ..., -0.05148177,\n",
       "        -0.07397635,  0.03995029],\n",
       "       [ 0.1282552 ,  0.02202352, -0.15690105, ..., -0.0061849 ,\n",
       "        -0.08959961, -0.02742513],\n",
       "       ..., \n",
       "       [ 0.07275391, -0.05419922, -0.375     , ..., -0.46679688,\n",
       "        -0.00466919,  0.06640625],\n",
       "       [-0.10681152, -0.14709473, -0.02877808, ...,  0.23095703,\n",
       "         0.14794922,  0.11889648],\n",
       "       [ 0.10205078, -0.32617188,  0.06103516, ...,  0.05615234,\n",
       "        -0.02233887, -0.27148438]], dtype=float32)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "triviaDataVecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing sentences from subtitles set\n",
      "Complete\n"
     ]
    }
   ],
   "source": [
    "sentences_tr = []\n",
    "\n",
    "print (\"Parsing sentences from subtitles set\")\n",
    "for text in df['subtitles']:    \n",
    "    sentences_tr += to_sentences(text, remove_stopwords = False)\n",
    "\n",
    "print (\"Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshi/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/__main__.py:9: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "clean_subs = []\n",
    "for text in df['subtitles']:\n",
    "    clean_subs.append(to_wordlist(text, remove_stopwords= False ))\n",
    "subsDataVecs = getAvgFeatureVecs(clean_subs, model, num_features = 300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.11791992, -0.03979492,  0.02087402, ..., -0.03656006,\n",
       "        -0.10400391, -0.08630371],\n",
       "       [ 0.15712193, -0.02580915,  0.04296875, ..., -0.19461496,\n",
       "        -0.09286063, -0.04375785],\n",
       "       [ 0.10078939, -0.02762858,  0.03930664, ..., -0.18058269,\n",
       "        -0.07623291, -0.06974157],\n",
       "       ..., \n",
       "       [ 0.02572632,  0.00257874,  0.10153198, ..., -0.03164291,\n",
       "        -0.01159668,  0.04003763],\n",
       "       [-0.13081868,  0.07226562, -0.01961263, ...,  0.04561361,\n",
       "        -0.07670084, -0.06654867],\n",
       "       [-0.03466797,  0.10928345,  0.02838135, ...,  0.01190567,\n",
       "         0.11981201, -0.07543945]], dtype=float32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subsDataVecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We got some true divide errors earlier, this would result is nan vector for the features of the reference and subtitles. So we fill these vectors with zeros to prevent disruptions in future analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subsDataVecs = np.nan_to_num(subsDataVecs)\n",
    "triviaDataVecs = np.nan_to_num(triviaDataVecs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we use cosine similarity to find out subtitle applies to what reference. Cosine similarity calculates the dot normalized dot project between two vectors. So for each referece, we go through the entire subtitles set to find similarities. The one with the maximum should be the one the applicable reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def trivia_to_subtitles(set_no):\n",
    "    \n",
    "    trivia_ = triviaDataVecs[set_no]\n",
    "    cosine_similarity = [0]*len(df['subtitles'])\n",
    "\n",
    "    for ii in range(0, len(df['subtitles'])):\n",
    "        if np.sum(subsDataVecs[ii]) == 0:\n",
    "            cosine_similarity[ii] = 0\n",
    "        else:\n",
    "            cosine_similarity[ii] = np.dot(trivia_, subsDataVecs[ii])/(np.linalg.norm(subsDataVecs[ii])* np.linalg.norm(trivia_))\n",
    "    \n",
    "    predicted = np.array(np.argwhere(cosine_similarity == np.amax(cosine_similarity)))\n",
    "    \n",
    "    predicted = predicted.flatten().tolist()\n",
    "    \n",
    "    print (trivia[set_no])    \n",
    "    pd.options.display.max_colwidth = 100\n",
    "    print (str(df['subtitles'][predicted]))\n",
    "    print (\"Start time: \", df[\"start_time\"][predicted])\n",
    "    print (\"End time: \", df['end_time'][predicted])\n",
    "    print ('------------------------------------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Wizard of Oz (1939) Lisa works as \"a Dorothy\" at the MGM Grand in Las Vegas and is therefore in a Judy Garland costume when she meets Mike and Trent.\n",
      "\n",
      "330    - Oh. Uh, Lisa works at the MGM Grand. - I'm a Dorothy.\n",
      "Name: subtitles, dtype: object\n",
      "Start time:  330    1226\n",
      "Name: start_time, dtype: int64\n",
      "End time:  330    1229\n",
      "Name: end_time, dtype: int64\n",
      "------------------------------------------------------------------------------------------------------\n",
      "You Bet Your Life (1950) (TV Series) Lorraine observes that Mike's business card has the duck logo from \"You Bet Your Life\".\n",
      "\n",
      "1232    Yeah, it's the logo from You Bet Your Life.\n",
      "Name: subtitles, dtype: object\n",
      "Start time:  1232    5034\n",
      "Name: start_time, dtype: int64\n",
      "End time:  1232    5037\n",
      "Name: end_time, dtype: int64\n",
      "------------------------------------------------------------------------------------------------------\n",
      "The Odd Couple (1968)\n",
      "\n",
      "550    One, two, three...\n",
      "Name: subtitles, dtype: object\n",
      "Start time:  550    2048\n",
      "Name: start_time, dtype: int64\n",
      "End time:  550    2050\n",
      "Name: end_time, dtype: int64\n",
      "------------------------------------------------------------------------------------------------------\n",
      "THX 1138 (1971)\n",
      "\n",
      "1199    - It's on. To you. - Jesus Christ. I'm sorry.\n",
      "Name: subtitles, dtype: object\n",
      "Start time:  1199    4741\n",
      "Name: start_time, dtype: int64\n",
      "End time:  1199    4744\n",
      "Name: end_time, dtype: int64\n",
      "------------------------------------------------------------------------------------------------------\n",
      "The Godfather (1972) When Trent walks into the casino, he talks about \"pulling a Fredo\". Referring to The Godfather when Moe Green tells Michael Corleone that Fredo was out of line for banging cocktail waitresses.\n",
      "\n",
      "397    He says, \"Trent, when you walked in this room,\n",
      "Name: subtitles, dtype: object\n",
      "Start time:  397    1437\n",
      "Name: start_time, dtype: int64\n",
      "End time:  397    1439\n",
      "Name: end_time, dtype: int64\n",
      "------------------------------------------------------------------------------------------------------\n",
      "Mean Streets (1973)\n",
      "\n",
      "676    All right, ready. Mean Streets?\n",
      "Name: subtitles, dtype: object\n",
      "Start time:  676    2435\n",
      "Name: start_time, dtype: int64\n",
      "End time:  676    2437\n",
      "Name: end_time, dtype: int64\n",
      "------------------------------------------------------------------------------------------------------\n",
      "Jaws (1975)\n",
      "\n",
      "861    These claws and these fangs! You're looking at your claws and your fangs.\n",
      "Name: subtitles, dtype: object\n",
      "Start time:  861    3213\n",
      "Name: start_time, dtype: int64\n",
      "End time:  861    3217\n",
      "Name: end_time, dtype: int64\n",
      "------------------------------------------------------------------------------------------------------\n",
      "Taxi Driver (1976)\n",
      "\n",
      "1363    I could've been out with Sue one night drinking and told her I was a race car driver.\n",
      "Name: subtitles, dtype: object\n",
      "Start time:  1363    5576\n",
      "Name: start_time, dtype: int64\n",
      "End time:  1363    5579\n",
      "Name: end_time, dtype: int64\n",
      "------------------------------------------------------------------------------------------------------\n",
      "Saturday Night Fever (1977) A nightclub band covers Staying Alive. Jon Favreau said that Swingers was intended to be a Saturday Night Fever for the 1990s.\n",
      "\n",
      "1260    She's the only one I danced with 'til tonight. That Lorraine is good.\n",
      "Name: subtitles, dtype: object\n",
      "Start time:  1260    5154\n",
      "Name: start_time, dtype: int64\n",
      "End time:  1260    5157\n",
      "Name: end_time, dtype: int64\n",
      "------------------------------------------------------------------------------------------------------\n",
      "Family Ties (1982) (TV Series)\n",
      "\n",
      "767    of Tina Yothers, you know, from Family Ties.\n",
      "Name: subtitles, dtype: object\n",
      "Start time:  767    2827\n",
      "Name: start_time, dtype: int64\n",
      "End time:  767    2829\n",
      "Name: end_time, dtype: int64\n",
      "------------------------------------------------------------------------------------------------------\n",
      "Star Wars: Episode VI - Return of the Jedi (1983)\n",
      "\n",
      "912    I'll show you who the fucking bitch is!\n",
      "Name: subtitles, dtype: object\n",
      "Start time:  912    3403\n",
      "Name: start_time, dtype: int64\n",
      "End time:  912    3406\n",
      "Name: end_time, dtype: int64\n",
      "------------------------------------------------------------------------------------------------------\n",
      "Staying Alive (1983) mentioned by a swinger\n",
      "\n",
      "792    We're staying alive Staying alive\n",
      "804    We're staying alive Staying alive\n",
      "Name: subtitles, dtype: object\n",
      "Start time:  792    2976\n",
      "804    3025\n",
      "Name: start_time, dtype: int64\n",
      "End time:  792    2978\n",
      "804    3027\n",
      "Name: end_time, dtype: int64\n",
      "------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/joshi/anaconda3/envs/tensorflow/lib/python3.5/site-packages/ipykernel/__main__.py:10: RuntimeWarning: invalid value encountered in float_scalars\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tapeheads (1988)\n",
      "\n",
      "Series([], Name: subtitles, dtype: object)\n",
      "Start time:  Series([], Name: start_time, dtype: int64)\n",
      "End time:  Series([], Name: end_time, dtype: int64)\n",
      "------------------------------------------------------------------------------------------------------\n",
      "Things Change (1988) When Trent refers to Mike as \"The guy behind the guy, behind the guy\", he's quoting the scene in Things Change where Jerry is making up a story explaining who Gino is.\n",
      "\n",
      "151    This is the guy behind the guy behind the guy.\n",
      "Name: subtitles, dtype: object\n",
      "Start time:  151    618\n",
      "Name: start_time, dtype: int64\n",
      "End time:  151    622\n",
      "Name: end_time, dtype: int64\n",
      "------------------------------------------------------------------------------------------------------\n",
      "Rain Man (1988)\n",
      "\n",
      "876    Bad man.\n",
      "Name: subtitles, dtype: object\n",
      "Start time:  876    3256\n",
      "Name: start_time, dtype: int64\n",
      "End time:  876    3258\n",
      "Name: end_time, dtype: int64\n",
      "------------------------------------------------------------------------------------------------------\n",
      "Goodfellas (1990) When Trent asks the cocktail waitress in Vegas to meet him at the Bamboo Lounge, this is a reference to a bar Tommy and Henry burnned down in Goodfellas; The scene where they go into the basement of the Derby is a recreation of the same type of scene in Goodfellas; When they are sitting around a table discussing movies, they talk about this forementioned scene.\n",
      "\n",
      "358    and there's all these little kids, uh, sitting in the waiting room with me.\n",
      "Name: subtitles, dtype: object\n",
      "Start time:  358    1320\n",
      "Name: start_time, dtype: int64\n",
      "End time:  358    1323\n",
      "Name: end_time, dtype: int64\n",
      "------------------------------------------------------------------------------------------------------\n",
      "Boyz n the Hood (1991)\n",
      "\n",
      "916    - What the fuck? - You asshole! Didn't you see Boyz N the Hood?\n",
      "Name: subtitles, dtype: object\n",
      "Start time:  916    3421\n",
      "Name: start_time, dtype: int64\n",
      "End time:  916    3424\n",
      "Name: end_time, dtype: int64\n",
      "------------------------------------------------------------------------------------------------------\n",
      "Reservoir Dogs (1992) When they are all sitting around talking about movies and teh camera is slowly panning around, this a recreation of the same type of shot in Reservoir Dogs.\n",
      "\n",
      "73    We're gonna spend half the night driving around the hills looking for this one party,\n",
      "Name: subtitles, dtype: object\n",
      "Start time:  73    341\n",
      "Name: start_time, dtype: int64\n",
      "End time:  73    344\n",
      "Name: end_time, dtype: int64\n",
      "------------------------------------------------------------------------------------------------------\n",
      "Glengarry Glen Ross (1992)\n",
      "\n",
      "185    Maybe a Glengow... Any Glen.\n",
      "Name: subtitles, dtype: object\n",
      "Start time:  185    761\n",
      "Name: start_time, dtype: int64\n",
      "End time:  185    763\n",
      "Name: end_time, dtype: int64\n",
      "------------------------------------------------------------------------------------------------------\n",
      "Star Trek: Deep Space Nine (1993) (TV Series)\n",
      "\n",
      "585    Dude, it's Pink Dot. Would you buzz him in? Hit nine.\n",
      "Name: subtitles, dtype: object\n",
      "Start time:  585    2148\n",
      "Name: start_time, dtype: int64\n",
      "End time:  585    2151\n",
      "Name: end_time, dtype: int64\n",
      "------------------------------------------------------------------------------------------------------\n",
      "Clerks (1994)\n",
      "\n",
      "534    Hey, Rob.\n",
      "Name: subtitles, dtype: object\n",
      "Start time:  534    1990\n",
      "Name: start_time, dtype: int64\n",
      "End time:  534    1992\n",
      "Name: end_time, dtype: int64\n",
      "------------------------------------------------------------------------------------------------------\n",
      "Pulp Fiction (1994)\n",
      "\n",
      "630    - What a surprise. - How novel.\n",
      "Name: subtitles, dtype: object\n",
      "Start time:  630    2278\n",
      "Name: start_time, dtype: int64\n",
      "End time:  630    2280\n",
      "Name: end_time, dtype: int64\n",
      "------------------------------------------------------------------------------------------------------\n",
      "Casino (1995)\n",
      "\n",
      "675    Wait. You gotta be nuts to shoot in a casino.\n",
      "Name: subtitles, dtype: object\n",
      "Start time:  675    2433\n",
      "Name: start_time, dtype: int64\n",
      "End time:  675    2435\n",
      "Name: end_time, dtype: int64\n",
      "------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for ii in range(len(trivia)):\n",
    "    trivia_to_subtitles(ii)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above results, we see that there are references that don't match up because IMDb lists just the movie name, when it could be applied to a particular song playing in the background, or action, type of directing, editing that applies to that scene. \n",
    "\n",
    "But, for those references that have explainations or are mentioned as names of movies, are prediction actually does very well. \n",
    "\n",
    "For future work, we'd like to produce cleaner data, and experiment with tfid and count vectorizer to hopefully yeild better results."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
